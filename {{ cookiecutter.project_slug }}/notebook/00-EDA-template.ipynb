{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ds_util.config import cfg, get_project_root\n",
    "from ds_util.data_accessors import snowflake_dataframe_from_sql, snowflake_execute_query\n",
    "\n",
    "#%matplotlib inline \n",
    "#import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pandas_profiling import ProfileReport\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = snowflake_dataframe_from_sql(\"\"\"\n",
    "select store_order_id,\n",
    "order_id,\n",
    "store_id,\n",
    "store_city,\n",
    "store_state \n",
    "from prod.reporting.store_orders_completed_core \n",
    "limit 1000\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_timestamp_columns = ['date', 'created_at','updated_at','dt','timestamp','event_time']\n",
    "for column in df.columns:\n",
    "    if str(column).lower() in common_timestamp_columns:\n",
    "        print(f\"Updating {column} from object to datetime\")\n",
    "        df[column] = pd.to_datetime(df[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic column information and counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data profiling\n",
    "This can be written to disk using the to_file(filename) command or it can be displayed directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = ProfileReport(df, explorative = True)\n",
    "pr.to_file(\"00-EDA-template-profile.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pairwise analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the default theme for seaborn\n",
    "sns.set_theme()\n",
    "sns.pairplot(df[df.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Great Expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import great_expectations as ge\n",
    "import great_expectations.jupyter_ux\n",
    "from great_expectations.data_context.types.resource_identifiers import ValidationResultIdentifier\n",
    "from great_expectations.dataset import (\n",
    "    PandasDataset,\n",
    "    MetaPandasDataset,\n",
    ")\n",
    "from great_expectations.profile import BasicSuiteBuilderProfiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert your dataframe into a GE dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge_df = ge.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the template \n",
    "The directory here is built by the command `great_expectations init,` but the cookiecutter template should have this already populated for you. This is where your expectations will get saved, and this is where Dagster will pull them from to run validation against flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ge.data_context.DataContext(context_root_dir=get_project_root().joinpath(\"dagster/great_expectations\"))\n",
    "# Name your suite whatever you want. This is the project template name\n",
    "expectation_suite_name = \"{{cookiecutter.project_slug}}.basic.warning\"\n",
    "# The arg overwrite_existing=True can be added if you want to overwrite stuff that is already there\n",
    "es = context.create_expectation_suite(expectation_suite_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure the data you will use for creating expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasource can be changed to be non-pandas (a local file, a file in s3, another database)\n",
    "# and the dataset will need to change accordingly\n",
    "batch_kwargs = {\n",
    "    'dataset': df,\n",
    "    'datasource': \"pandas\"\n",
    "}\n",
    "batch = context.get_batch(batch_kwargs, expectation_suite_name)\n",
    "#Test that the batch has data and see what it is\n",
    "batch.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start to explore expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "included_columns = [\n",
    "    \"store_order_id\",\n",
    "    \"order_id\",\n",
    "    \"store_id\",\n",
    "    \"store_city\",\n",
    "    \"store_state\",\n",
    "]\n",
    "scaffold_config = {\n",
    "    \"included_columns\" : included_columns\n",
    "}\n",
    "# This generates a bunch of expectations against your data that generally are bad, \n",
    "# but they give you a format to work against\n",
    "suite, evr = BasicSuiteBuilderProfiler().profile(batch, profiler_configuration=scaffold_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few sample explorations to get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.expect_column_quantile_values_to_be_between(\n",
    "    column=\"store_order_id\",\n",
    "    allow_relative_error=False,\n",
    "    quantile_ranges={\n",
    "        \"quantiles\": [0.05, 0.25, 0.5, 0.75, 0.95],\n",
    "        \"value_ranges\": [\n",
    "            [39.0, 41.0],\n",
    "            [127.0, 129.0],\n",
    "            [391.0, 393.0],\n",
    "            [1245.0, 1247.0],\n",
    "            [5303.0, 5305.0],\n",
    "        ],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.expect_column_values_to_not_be_null(column=\"order_id\", mostly=0.898)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.expect_column_values_to_not_be_in_set(column=\"store_state\", value_set=[\"fakeville\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist the expectations\n",
    "context.save_expectation_suite(suite, expectation_suite_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test expectations by running validate against a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate\n",
    "results = context.run_validation_operator(\"action_list_operator\", assets_to_validate=[batch])\n",
    "\n",
    "# build the documentation for the result\n",
    "validation_result_identifier = results.list_validation_result_identifiers()[0]\n",
    "context.build_data_docs()\n",
    "data_docs_urls = context.get_docs_sites_urls(\n",
    "            resource_identifier=validation_result_identifier,\n",
    "            only_if_exists=False,\n",
    "        )\n",
    "urls_to_open = [site[\"site_url\"] for site in data_docs_urls]\n",
    "url = urls_to_open[0].replace('file:///work/', 'http://localhost:8888/view/')\n",
    "\n",
    "# You'll need to paste this in your browser if you are running this in the container\n",
    "# if needed you can add `+f'?token={access_token}' to the end of the above line where the access token \n",
    "# is generated by your notebook server \n",
    "url"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
